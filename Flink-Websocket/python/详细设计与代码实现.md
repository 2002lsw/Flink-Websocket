# 3 详细设计与代码实现

## 3.1 Kafka环境搭建

1.在配置Kafka之前，我们采用ZooKeeper作为管理存储和管理kafka集群元数据

### 2.安装ZooKeeper

首先，进入Zookeeper官网（https://zookeeper.apache.org/），下载你所需要的ZooKeeper版本，本设计采用的是3.6.3版本

![img](file:///C:\Users\22121\AppData\Local\Temp\ksohtml21228\wps1.jpg) 

然后，将下载下来的ZooKeeper解压到本地路径，进入ZooKeeper的conf目录下，复制zoo_sample.cfg配置文件，将其命名为zoo.cfg

其中，zoo.cfg配置文件中各配置项的含义如下所示：

| # zookeeper时间配置中的基本单位（毫秒）                      | tickTime=2000                       |
| ------------------------------------------------------------ | ----------------------------------- |
| # 允许follower初始化连接到leader最大时长，它表示tickTime时间的倍数 即： | initLimit*tickTime initLimit=10     |
| # 运行follower与leader数据同步最大时长，它表示tickTime时间倍数 | syncLimit*tickTime syncLimit=5      |
| # zookeeper数据存储目录及日志保存目录（如果没有指明dataLogDir，则日志也保存在这个文件中） | dataDir=/tmp/zookeeper              |
| # 对客户端提供的端口号                                       | clientPort=2181                     |
| # 单个客户端于zookeeper最大并发连接数                        | maxClientCnxns=60                   |
| # 保存的数据快照数量，之外的将会被清除                       | autopurge.snapRetainCount=3         |
| # 自动出发清除任务时间间隔，以小时为单位。默认为0，表示不自动清除 | autopurge.purgeInterval=1           |
| #metricsProvider.httpPort=7000                               | #metricsProvider.exportJvmInfo=true |
| ## ttl settings                                              | extendedTypesEnabled=true           |

配置好zoo.conf配置文件后，我们就可以启动ZooKeeper了，我们也可以通过调用zkServer.sh status命令，来查看zookeeper的运行状态。

## 3.2 Kafka jar包下载

安装包下载链接  http://kafka.apache.org/downloads.html 

下载所需要的版本，该设计使用的是kafka_2.12-3.2.0

![img](file:///C:\Users\22121\AppData\Local\Temp\ksohtml21228\wps2.jpg) 

## 3.3 Kafka安装部署

## 3.3.1 解压安装包

本系统统一将kafka安装到/D:\App目录下

## 3.3.2 配置内容

| #broker的全局唯一编号，不能重复     | broker.id=0                         |
| ----------------------------------- | ----------------------------------- |
| #删除topic功能使能                  | delete.topic.enable=true            |
| #处理网络请求的线程数量             | num.network.threads=3               |
| #用来处理磁盘IO的现成数量           | num.io.threads=8                    |
| #发送套接字的缓冲区大小             | socket.send.buffer.bytes=102400     |
| #接收套接字的缓冲区大小             | socket.receive.buffer.bytes=102400  |
| #请求套接字的缓冲区大小             | socket.request.max.bytes=104857600  |
| #kafka运行日志存放的路径            | log.dirs=/opt/module/kafka/logs     |
| #topic在当前broker上的分区个数      | num.partitions=1                    |
| #用来恢复和清理data下数据的线程数量 | num.recovery.threads.per.data.dir=1 |
| #segment文件保留的最长时间          | log.retention.hours=168             |

## 3.3.4 配置系统文件

\#此服务器的角色。设置这个值将使我们置于KRaft模式

process.roles=broker,controller

\#与此实例的角色关联的节点

idnode.id=1

\#连接字符串的控制器仲裁controller.quorum.voters=1@localhost:9093

\#套接字服务器所监听的地址。

\#组合节点（即那些具有`process.roles=broker、控制器`的节点）必须在这里至少列出控制器侦听器。

\#如果未定义代理侦听器，则默认侦听器将使用一个等于java.net值的主机名。InetAddress.getCanonicalHostName(),# with PLAINTEXT listener name, and port 9092.

\#格式：listeners = listener_name://host_name:port

示例：listeners = PLAINTEXT://your.host.name:9092

listeners=PLAINTEXT://:9092,CONTROLLER://:9093

\#用于代理之间的通信的侦听器的名称。

inter.broker.listener.name=PLAINTEXT

\#监听器名称、主机名和端口，代理将向客户机做广告。

advertised.listeners=PLAINTEXT://localhost:9092

\#控制器使用的侦听器名称的以逗号分隔的列表。

\#如果在KRaft模式下运行，这是必需的。

controller.listener.names=CONTROLLER

\#将侦听器名称映射到安全协议，默认值为它们是相同的。有关更多细节，请参见配置文档

listener.security.protocol.map=CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL

\#服务器用于接收来自网络的请求和向网络发送响应的线程数num.network.threads=3

\#服务器用于处理请求的线程数，其中可能包括磁盘I/O

num.io.threads=8

\#套接字服务器所使用的发送缓冲区（SO_SNDBUF）

socket.send.buffer.bytes=102400

\#套接字服务器所使用的接收缓冲区（SO_RCVBUF）

socket.receive.buffer.bytes=102400

\#套接字服务器将接受的请求的最大大小（针对OOM的保护）

socket.request.max.bytes=104857600

日志基础知识

\#一个以逗号分隔的存储日志文件的目录列表

log.dirs=/tmp/kraft-combined-logs

\#每个主题的默认日志分区数。更多的分区允许更大的

\#并行性的消费

num.partitions=1

\#在启动时用于日志恢复和关闭时刷新的每个数据目录的线程数。

\#对于数据目录位于RAID阵列中的安装，建议增加此值。

num.recovery.threads.per.data.dir=1

内部主题设置

The replication factor for the group metadata internal topics "__consumer_offsets" and "__transaction_state"

\#对于开发测试之外的任何值，建议大于1以确保可用性，如3。

offsets.topic.replication.factor=1

transaction.state.log.replication.factor=1

transaction.state.log.min.isr=1

日志刷新策略

\#消息会立即写入文件系统#操作系统缓存延迟。以下配置控制数据到磁盘的刷新。

这里有几个重要的权衡之处：

\# 1.持久性：如果您不使用复制，未刷新的数据可能会丢失。

\# 2.延迟：当刷新确实发生时，非常大的刷新间隔可能会导致延迟峰值，因为将会有大量的数据需要刷新。

\# 3.吞吐量：刷新通常是最昂贵的操作，并且较小的刷新间隔可能会导致过多的搜索。

\#下面的设置允许人们配置刷新策略，以便在一段时间后刷新数据或

\#每N条消息（或两者都有）。这可以全局完成，并在每个主题上覆盖。

\#在强制将数据刷新到磁盘之前要接受的消息数

\#log.flush.interval.messages=10000

\#在强制刷新之前，消息在日志中的最长时间

\#log.flush.interval.ms=1000

日志保留策略

\#以下配置控制日志段的处理。

\# 设置为在一段时间后或在给定大小累积之后删除段。

\# 只要满足这些条件，一个段就将被删除。

\#从日志的末尾开始。

\# 由于年龄而有资格被删除的日志文件的最小年龄

log.retention.hours=168

\# 一种基于大小的日志保留策略。

\# 日志段文件的最大大小。当达到此大小后，将创建一个新的日志段。log.segment.bytes=1073741824

\#日志段被检查的时间间隔，看看它们是否可以根据

log.retention.check.interval.ms=300000

## 3.3.5 启动Kafka

![img](file:///C:\Users\22121\AppData\Local\Temp\ksohtml21228\wps3.jpg) 

并通过查看kafka的进程来判断是否启动。

## 3.4 Spark环境搭建

访问链接[Documentation | Apache Spark](https://spark.apache.org/documentation.html) 下载需要的Spark版本

1．官网地址

http://spark.apache.org/

2．文档查看地址

https://spark.apache.org/docs/2.1.1/

3．下载地址

https://spark.apache.org/downloads.html

（1）基本语法

```
bin/spark-submit

 \--class <main-class>

--master <master-url> 

\--deploy-mode <deploy-mode> \

--conf <key>=<value>

 \... # other options<application-jar> 

\[application-arguments]
```

（2）参数说明：

```
--master 指定Master的地址，默认为Local

--class: 你的应用的启动类 (如 org.apache.spark.examples.SparkPi)

--deploy-mode: 是否发布你的驱动到worker节点(cluster) 或者作为一个本地客户端 (client) (default: client)*

--conf: 任意的Spark配置属性， 格式key=value. 如果值包含空格，可以加引号“key=value” 

application-jar: 打包好的应用jar,包含依赖. 这个URL在集群中全局可见。 比如hdfs:// 共享存储系统， 如果是 file:// path， 那么所有的节点的path都包含同样的jar

application-arguments: 传给main()方法的参数

--executor-memory 1G 指定每个executor可用内存为1G

--total-executor-cores 2 指定每个executor使用的cup核数为2个
```

解压缩到对应目录下，完成python进程和Java进程的通信，使用python语言做大数据分析。

把中间件文件（pyspark,py4j）放入python相关目录

![img](file:///C:\Users\22121\AppData\Local\Temp\ksohtml21228\wps4.jpg) 

调式环境是否配置完成，完成后如下图

![img](file:///C:\Users\22121\AppData\Local\Temp\ksohtml21228\wps5.jpg) 

## 3.5 watchdog层代码

看门狗是用Python编写的文件系统监视工具，具体代码如下

```
#!/usr/bin/python
# -*- coding: utf-8 -*-
import re
import os
import tempfile

class Properties:
    def __init__(self, file_name):
        self.file_name = file_name
        self.properties = {}
           try:
            fopen = open(self.file_name, 'r')
            for line in fopen:
                line = line.strip()
                if line.find('=') > 0 and not line.startswith('#'):
                    strs = line.split('=')
                    self.properties[strs[0].strip()] = strs[1].strip()
        except :
            pass
        else:
            fopen.close()
    def has_key(self, key):
        if key in self.properties:
            return True
        else:
            return False

    def get(self, key, default_value=''):
        if key in self.properties:
            return self.properties[key]
        return default_value

    def put(self, key, value):
        self.properties[key] = value
            replace_property(self.file_name, key + '=.*', key + '=' + value, True)

def parse(file_name):
    return Properties(file_name)
def replace_property(file_name, from_regex, to_str, append_on_not_exists=True):
    file = tempfile.TemporaryFile()         #创建临时文件

    if os.path.exists(file_name):
        r_open = open(file_name,'r')
        pattern = re.compile(r'' + from_regex)
        found = None
        for line in r_open: #读取原文件
            if pattern.search(line) and not line.strip().startswith('#'):
                found = True
                line = re.sub(from_regex, to_str, line)
            file.write(line)   #写入临时文件
        if not found and append_on_not_exists:
            file.write('\n' + to_str)
        r_open.close()
        file.seek(0)
        content = file.read()  #读取临时文件中的所有内容
        if os.path.exists(file_name):
            os.remove(file_name)
               w_open = open(file_name,'w')
        w_open.write(content)   #将临时文件中的内容写入原文件
        w_open.close()
        file.close()  #关闭临时文件，同时也会自动删掉临时文件
    else:
        print ("file %s not found" % file_name)

```

## 3.6调用watchdog监测日志文件的变化：

```
from wat
chdog.events import *  # 同上
from watchdog.observers import Observer  # WatchDog相关的模块，用来做文件监控
import subprocess
from multiprocessing import Process
import time
import properties
import codecs # Chinese unicode transfer
from kafka import KafkaProducer
class FileEventHandler(FileSystemEventHandler):
    def __init__(self):
        FileSystemEventHandler.__init__(self)
            def on_moved(self, event):
        pass
    def on_created(self, event):  # 逻辑代码是base在当文件新增的时候进行开发
        if event.is_directory:
            pass
        else:
            print("file created:{0}".format(event.src_path))
    def on_deleted(self, event):
        pass
    def on_modified(self, event):
        if event.is_directory:
            pass
        else:
            if not re.search('.filepart',event.src_path) and  not re.search('backup',event.src_path):
                print(event.src_path+"has modifided!")
                #FILE_MODIFY_PROCESS = Process(target=SEND_TXN_LOG_KAFKA,args=(event.src_path, 'log-pv-uv'))
                #FILE_MODIFY_PROCESS.start()
def SEND_TXN_LOG_KAFKA(src_file_path,topic):
    prop_path='config.properties'
    props = properties.parse(prop_path)
    kafka_addr = props.get('kafka_address')
    producer = 
    KafkaProducer(bootstrap_servers=kafka_addr,value_serializer=lambda m: bytes(m, encoding="gb2312"))
    try:
        with codecs.open(src_file_path, 'rb',encoding='gb2312') as f:
            context = f.readlines()
            for eachline in context:
                print(eachline)
                producer.send(topic, eachline)
        f.close()
    except Exception as e:
        raise e
        Logger("RUN kafka  ERROR....")
        print("RUN kafka  ERROR....")
    finally:
        Pass
def FILE_HANDLER_PROCESS(file_path,bool):
    observer = Observer()  # 文件监控开始
    event_handler = FileEventHandler()  # 启动一个线程文件事件
    observer.schedule(event_handler, file_path, bool)  
# 对指定的文件夹进行监控，当文件更新后触发一些操作 
#observer.schedule(event_handler,"E:\\APP\\Python_script\\SEMIR_REALTIME_ETL_PROCCESS\\dist\\LOG",True)
    observer.start() 
 # 文件线程 监控开始s
    try:
         while True:
            time.sleep(1)
    except KeyboardInterrupt:
        observer.stop()
        time.sleep(1)
    finally:
        observer.start()
    #observer.join()
if __name__ == '__main__':
    log_monitor_process = Process(target=FILE_HANDLER_PROCESS,args=('D:\\APP\\upload\\',True))
    log_monitor_process.start()
    log_monitor_process.join()
```

## 3.7导入模块 pyspark

```
from pyspark.sql import SQLContext, Row # 用这个工具箱进行sql分析
# -*- coding: utf-8 -*-
from pyspark.sql import SparkSession
from pyspark.sql.functions import from_json, window, lit
from pyspark.sql.types import StructType, StringType
# 导入系统模块
import os
#import pymysql
import time
from datetime import datetime # 对日期进行处理
#import exrex
if __name__ == '__main__':
    # 设置SPARK_HOME环境变量, 非常重要, 如果没有设置的话,SparkApplication运行不了
    # 设置开始时间
    os.environ['SPARK_HOME'] = 'D:\\App\\spark\\'
    os.environ['HADOOP_HOME']= 'D:\\App\\hadoop\\winuntil\\'
    # Create SparkConf
    spark = SparkSession \
        .builder \
        .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.7") \
        .appName("pyspark_structured_streaming_kafka") \
        .getOrCreate()
在使用pyspark编程运行程序时,依赖于py4j模块，为什么？？？？
由于在运行的时候，SparkContext Python API创建的对象 通过 py4j(python for java)转换为JavaSparkContext，然后调度Spark Application.
    # 设置日志级别
    df = spark.readStream \
        .format("kafka") \
        .option("kafka.bootstrap.servers", "localhost:9092") \
        .option("partition.assignment.strategy", "range")\
        .option("subscribe",'log-pv-uv') \
           .load()
    base_df = df.selectExpr("CAST(value AS STRING)")
    from pyspark.sql.functions import regexp_extract
    split_df = base_df.select(
        regexp_extract('value', r'^([^\s]+\s)', 1).alias("host"),
        regexp_extract('value', r'^.*\[(\d\d/\w{3}/\d{4}:\d{2}:\d{2}:\d{2} \+\d{4})]', 1).alias('timestamp'),
        regexp_extract('value', r'^.*"\w+\s+([^\s]+)\s+HTTP.*"', 1).alias('path'),
        regexp_extract('value', r'^.*"\s+([^\s]+)', 1).cast('integer').alias('status'),
        regexp_extract('value', r'^.*\s+(\d+)$', 1).cast('integer').alias('content_size')
    )
    print(split_df)
    #split_df.show(20,truncate=False)

    def parse_clf_time(s):
        # 定义一个字典
        month_map = {
            'Jan': 1, 'Feb': 2, 'Mar': 3, 'Apr': 4, 'May': 5, 'Jun': 6,
            'Jul': 7, 'Aug': 8, 'Sep': 9, 'Oct': 10, 'Nov': 11, 'Dec': 12
        }
        return "{0:04d}-{1:02d}-{2:02d} {3:02d}:{4:02d}:{5:02d}".format(
            int(s[7:11]),
            month_map[s[3:6]],
            int(s[0:2]),
                 int(s[12:14]),
            int(s[15:17]),
            int(s[18:20]))
    from pyspark.sql.functions import udf
    u_parse_time = udf(parse_clf_time)
    #进行数据转换
    logs_df = split_df.select(
        '*',
        u_parse_time(split_df['timestamp']).cast('timestamp').alias('time')
    ).drop('timestamp')
    # # 打印
    from pyspark.sql.functions import dayofmonth,hour,minute
    from pyspark.sql.functions import col, sum,count,from_unixtime
    hour_to_host_pair_df =logs_df.select(logs_df.host, hour(logs_df.time).alias('hour'))
    hour_group_host_df = hour_to_host_pair_df.distinct()
    hour_host_df = hour_group_host_df.groupBy('hour').count()
    # select data only inclue hosts and timestamp
    raw_date_hosts = logs_df.select(logs_df.time,logs_df.host)
    stat_hosts404 = logs_df.filter('status=404').select(logs_df.time,logs_df.host)
    # 定义连接参数
    mysql_host = "192.168.1.182"
    mysql_port = "3306"
    mysql_user = "root"
     mysql_password = "conqure2013"
    mysql_database = "biprod_realtime"
    # 定义输出模式为 append 或 update
    output_mode = "append"
    windowedCounts = raw_date_hosts \
    .withWatermark("time", "5 minutes")\
        .groupBy(window(col("time"), "5 minutes", "5 minutes")) \
        .agg(count("host").alias("count"))
    windowedCounts404 = stat_hosts404 \
        .withWatermark("time", "5 minutes") \
        .groupBy(window(col("time"), "5 minutes", "5 minutes")) \
        .agg(count("host").alias("count"))
    # processedDF = windowedCounts.select(
    #     col("window.start").alias("start"),
    #     col("window.end").alias("end"),
    #     col("count")
    # ).withColumn("start_string", from_unixtime(col("start")).cast("string")).withColumn("end_string", from_unixtime(col("end")).cast("string"))
    midWindowCount = windowedCounts.withColumn("endwindow",col("window").getField("end"))
    finalWindowCount = midWindowCount.select(col("endwindow"),col("window").cast("string").alias("window"), col("count"))
    midWindowCount404 = windowedCounts404.withColumn("endwindow",col("window").getField("end"))
        finalWindowCount404 = midWindowCount404.select(col("endwindow"),col("window").cast("string").alias("window"), col("count"))
    # 定义写入 MySQL 的操作
    def write_to_mysql(batch_df, batch_id,table_name):
        batch_df_id = batch_df.withColumn("batch_id",lit(batch_id))
        batch_df_id.write.format("jdbc").option("url", f"jdbc:mysql://{mysql_host}:{mysql_port}/{mysql_database}?useSSL=false").option("dbtable", table_name).option("user", mysql_user).option("password", mysql_password).mode(output_mode).save()
    # 启动流式查询
    query = finalWindowCount.writeStream.outputMode('complete').foreachBatch(lambda batch_df, batch_id: write_to_mysql(batch_df, batch_id, "client_access_log_window_level_count")).start()
    query404 = finalWindowCount404.writeStream.outputMode('complete').foreachBatch(lambda batch_df, batch_id: write_to_mysql(batch_df, batch_id, "client_access_log_404err_window_level_count")).start()
    #query = windowedCounts.writeStream.outputMode('complete').format('console').start()
    query.awaitTermination()
    query404.awaitTermination()
```

## 3.8连接数据库

```
import pymysql
class Mysql(object):
    def __init__(self):
        try:
            self.conn = pymysql.connect(host='192.168.1.182',user='root',password='conqure2013',database='biprod_realtime',charset="utf8")
            self.cursor = self.conn.cursor()  # 用来获得python执行Mysql命令的方法（游标操作）
            print("连接数据库成功")
        except:
            print("连接失败")
    def getItems(self):
        sql= '''select  *from  client_access_log_window_level_count
where batch_id =(select max(batch_id) from client_access_log_window_level_count)
order by endwindow '''    #获取food数据表的内容
        self.cursor.execute(sql)
        items = self.cursor.fetchall()  #接收全部的返回结果行
        return items
```

# 4 项目演示

## 4.1生成日志文件

步骤1检查java基础环境，确定是1.8版本（文件保存在D：[\\App](http://App)里面，具体路径以自己保存地址为准） 

步骤2：访问log_gen CMD窗口输入： java -jar nginx_log_gen.jar D:\\App\\upload\\ 

步骤3：提示日志生成完成即表示成功，如生成失败，请检查路径是否正确

![img](file:///C:\Users\22121\AppData\Local\Temp\ksohtml21228\wps6.jpg) 

## 4.2 启动Zookeeper

步骤1：打开cmd窗口（以管理员身份运行1），进入到D：\App\kafka 目录下，输入：

```
bin\windows\zookeeper-server-start.bat config\zookeeper.properties
```

![img](file:///C:\Users\22121\AppData\Local\Temp\ksohtml21228\wps7.jpg) 

## 4.3 启动kafka

步骤1：打开cmd窗口，在D：\App\kafka 目录下输入：

```
 bin\windows\kafka-server-start.bat config\server.properties
```

![img](file:///C:\Users\22121\AppData\Local\Temp\ksohtml21228\wps8.jpg) 

## 4.4 启动watchdog

步骤1：打开cmd窗口 在D：\App\watchdog 目录下输入：

```
python log_realtime_capture_kafka.py
```

![img](file:///C:\Users\22121\AppData\Local\Temp\ksohtml21228\wps9.jpg)					

输出D:\App\upload\nginx_loghas modifided! 即表示监测到日志文件

## 4.5 消费数据

步骤1：打开cmd窗口，在D：\App\kafka 目录下输入：

```
bin\windows\kafka-console-consumer.bat --bootstrap-server localhost:9092  --topic log-pv-uv											
```

![img](file:///C:\Users\22121\AppData\Local\Temp\ksohtml21228\wps10.jpg) 

​											

## 4.6 建立数据库表

步骤1：连接数据库，在D：\App\rt_analysis 目录下，找到对应的python文件

![img](file:///C:\Users\22121\AppData\Local\Temp\ksohtml21228\wps11.jpg) 

输入数据库对应的账号密码和IP地址，建立连接。

步骤2：打开数据库软件，将数据库包导入并运行，生成表

![img](file:///C:\Users\22121\AppData\Local\Temp\ksohtml21228\wps12.jpg) 

## 4.7 数据清洗，传入数据库

步骤1：配置python环境，需保证在3.7.3的版本下运行

步骤2：配置spark环境

步骤3：打开cmd窗口，在D：\App\rt_analysis 目录下输入：

```
python  uv_log_analysis_rt.py
```

![img](file:///C:\Users\22121\AppData\Local\Temp\ksohtml21228\wps13.jpg) 

未出现报错提示，即表示程序运行成功，需等待一会；如出现报错，请检查数据文件名与python代码里的路径是否一致

步骤5:刷新数据库，监测client_access_log_window_level_count表是否有数据传入，成功如图4-7-2所示

![img](file:///C:\Users\22121\AppData\Local\Temp\ksohtml21228\wps14.jpg) 

步骤6：打开数据库，运行数据库语句，查找数据库语句。

![img](file:///C:\Users\22121\AppData\Local\Temp\ksohtml21228\wps15.jpg) 

​														图4-7-3 查找数据库语句

选择以Notepad++编辑打开或任意python运行软件，即可查看（黄色字体语句）

![img](file:///C:\Users\22121\AppData\Local\Temp\ksohtml21228\wps16.jpg) 

步骤7：运行步骤6语句即可将传入的数据进行排序1.新建查询 2.运行即可；如图即可对数据进行整理

![img](file:///C:\Users\22121\AppData\Local\Temp\ksohtml21228\wps17.jpg) 

## 4.8 启动echarts

步骤1：连接数据库，打开cmd窗口，在D：//App//flask_reltime_echarts 目录下输入：

```
python Flask.py
```

步骤2：运行结果如图

![img](file:///C:\Users\22121\AppData\Local\Temp\ksohtml21228\wps18.jpg) 

提示数据库连接成功即正确

步骤3：获取网站地址

![img](file:///C:\Users\22121\AppData\Local\Temp\ksohtml21228\wps19.jpg) 

步骤4：打开浏览器，输入复制到网址+echarts回车跳转

http://127.0.0.1:5000/echarts

跳转到图即表示程序运行成功！！ 

![img](file:///C:\Users\22121\AppData\Local\Temp\ksohtml21228\wps20.jpg) 

